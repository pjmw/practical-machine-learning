---
title: "Peer-graded Assignment: Prediction Assignment Writeup"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

## Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 
```{r}
library(caret)
library(rpart)
library(dplyr)

set.seed(1)
training <- read.csv("pml-training.csv", na.strings=c("", "NA", "#DIV/0!"), row.names = 1)
testing <- read.csv("pml-testing.csv", na.strings=c("", "NA", "#DIV/0!"), row.names = 1)
training <- tbl_df(training)
testing <- tbl_df(testing)
```
The original file contains 159 columns with over 19000 rows. We will remove columns with missing values:
```{r}
training <- training[,colSums(is.na(training)) == 0]
testing <- testing[,colSums(is.na(testing)) == 0]
```
Also we will remove unnecessary columns such as the first 6 columns:
```{r}
training <- training[,-c(1:6)]
testing <- testing[,-c(1:6)]
dim(training)
dim(testing)
```
We split the training set into subtraining and subtesting sets in order to allow cross-validation:
```{r}
index <- createDataPartition(y=training$classe, p=0.8, list=FALSE)
trainingData <- training[index, ]
testingData <- training[-index, ]
```
The `classe` variable is a factor variable with 5 levels, A to E all of which occur in the same order of magnitude, as the following plot shows:
```{r}
plot(trainingData$classe, main="levels of  `classe` within the subtraining set", xlab="levels", ylab="frequency")
```

# Model selection
We try two different models, one based on decision trees, the other one random forest.

## Decision trees
Our first model is by using decision trees:
```{r}
model1 <- rpart(classe ~ ., data=trainingData, method="class")
prediction1 <- predict(model1, testingData, type="class")
confusionMatrix(prediction1, testingData$classe)
```
The accuracy of the decision tree method is at **74 %** which is very low.

## Random Forest
Next, we try the random forest algorithm:
```{r}
#model2 <- train(classe ~ ., data=trainingData, method="rf")  # takes forever
model2 <- train(classe ~ ., data=trainingData, method="rf", trControl=trainControl(method="none"), tuneGrid=data.frame(mtry=7)) # note by Mauricio Collaca on Coursera forum
#model2 <- randomForest(classe ~ ., data=trainingData) # alternative method
```
The in sample error rate is given by:
```{r}
prediction2 <- predict(model2, trainingData)
confusionMatrix(prediction2, trainingData$classe)
```
which is clear since we used the same data set. The out-of-sample error rate must be less:
```{r}
prediction2 <- predict(model2, testingData)
confusionMatrix(prediction2, testingData$classe)
```
A value of over **99.5%** of accuracy is very good. We will use this model to predict our 20 test cases:
```{r}
prediction3 <- predict(model2, testing)
prediction3
```
All cases are predicted correctly.
